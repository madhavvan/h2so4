
export const INITIAL_RESUME_TEXT = `
VENU MADHAV PENTALA 
Data Engineer | Cloud Data Engineer | ETL Developer
pentalavenumadhav@gmail.com | +1 (317) 955-9198 | Indianapolis, IN | LinkedIn | GitHub

PROFESSIONAL SUMMARY 
Cloud Data Engineer with 4+ years of extensive experience building efficient and robust ETL pipelines, advanced analytics solutions, and data orchestration frameworks using Python, SQL, Apache Spark, AWS Glue, Azure Data Factory, GCP and Terraform. Demonstrated success improving processing performance by 94%, reducing error rates by 25%, and integrating multi-cloud environments. Holds multiple certifications (AWS Solutions Architect, Azure Data Engineer, Databricks Certified Data Engineer). Adept in Agile methodologies, team leadership, and stakeholder collaboration, committed to delivering scalable, compliant data engineering solutions.

TECHNICAL SKILLS 
Programming Languages: Python, SQL, PL/SQL, R, SAS, Java, JavaScript, Kotlin, Scala
Databases: PostgreSQL, MySQL, MongoDB, Snowflake, Cassandra, DynamoDB, Oracle
Data Engineering Tools: Apache Airflow, Apache Spark, Apache Kafka, dbt, Apache NiFi, AWS Glue, Talend, Delta Lake, Trino, Jupyter Notebook, Alteryx, UiPath, LangChain
Data Visualization: Tableau, ThoughtSpot, Power BI, Plotly, Seaborn, Matplotlib
Data App Development: Streamlit, Dash.
Cloud Platforms: AWS (S3, Glue, Lambda, Redshift, Kinesis, EMR), Azure (Data Factory, Synapse), Google Cloud (BigQuery, Dataflow), Docker, Kubernetes, Terraform
Big Data Technologies: Hadoop, Spark, Hive, Apache Flink
DevOps & Version Control: Git, GitHub, Jenkins, Azure DevOps, GitLab CI/CD

PROFESSIONAL EXPERIENCE 
Data Engineer (Intern) | G Technologies, Indianapolis, IN | May 2025 – Present
- Designed cloud-native ETL pipelines using Apache NiFi, AWS Glue, and Delta Lake for high-volume batch processing.
- Integrated data across multi-cloud environments (AWS, GCP) using Terraform-based infrastructure as code.
- Leveraged Trino for federated query optimization, reducing cross-database latency by 40%.
- Optimized cloud resource utilization with Kubernetes and Docker, increasing deployment efficiency by 35%.

Data Engineer – Research Assistant| Indiana University, Indianapolis, IN, USA | January 2024 – Present
- Developed robust ETL pipelines using Apache Airflow, Python, and SQL, processing 50K+ records daily with 30% lower latency.
- Refined Snowflake models using star schema and Oracle integration to improve analytical performance by 25%.
- Automated data movement using AWS Lambda, S3 & Terraform, ensuring 99% accuracy.

Data Engineer | Apollo Hospitals, Hyderabad, Telangana, India | May 2023 – December 2023
- Reduced processing time by 94% by engineering real-time ETL pipelines with Apache Kafka and Python, processing 3M+ records/day.
- Implemented automated data-quality validation checks, achieving near-zero downtime.
- Modeled healthcare data in PostgreSQL and Oracle, improving query performance by 20%.

Data Engineer (Intern) | Nizam’s Institute of Medical Sciences | October 2022 – April 2023
- Developed Apache Spark pipelines for 100K+ clinical records, improving data accessibility by 35%.
- Built MongoDB and Oracle hybrid data models, reducing response time for reporting use cases by 20%.

Data Engineer (Contract) | KIMS Hospitals, Hyderabad | June 2016 - August 2017
- Architected PostgreSQL ETL pipelines, processing 75K+ records daily.
- Enhanced query performance by 22% using Snowflake schema modeling.

EDUCATION 
Master of Science in Health Informatics | Indiana University, Indianapolis, IN | 2025 | GPA: 3.8/4.0 
Bachelor of Science in Physiotherapy | Nizam’s Institute of Medical Sciences, Hyderabad, India | 2022

CERTIFICATIONS 
AWS Certified Solutions Architect – Associate | June 2023
Microsoft Certified: Azure Data Engineer Associate (DP-203) | December 2023
Databricks Certified Data Engineer Associate | March 2024
Microsoft Certified: Power BI Data Analyst Associate | October 2022

PROJECTS 
AI Data Analysis Platform with TensorFlow & Streamlit | DataToyAI.com
Real-Time ETL Pipeline Web App (Healthcare Interoperability)
AsthmaCare – Pediatric Clinical Decision Support App
Thyroid Disease Prediction Engine
Autply – AI-Powered WhatsApp Chatbot
`;

export const INITIAL_JD_TEXT = `
What We Do
At Goldman Sachs, our Engineers don’t just make things – we make things possible. Change the world by connecting people and capital with ideas. Solve the most challenging and pressing engineering problems for our clients. Join our engineering teams that build massively scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action. Create new businesses, transform finance, and explore a world of opportunity at the speed of markets.

Engineering, which is comprised of our Technology Division and global strategists groups, is at the critical center of our business, and our dynamic environment requires innovative strategic thinking and immediate, real solutions. Want to push the limit of digital possibilities? Start here.

Who We Look For
Goldman Sachs Engineers are innovators and problem-solvers, building solutions in risk management, big data, mobile and more. We look for creative collaborators who evolve, adapt to change and thrive in a fast-paced global environment.

Asset & Wealth Management Engineering 
Across Wealth Management, Goldman Sachs helps empower clients and customers around the world to reach their financial goals. Our advisor-led wealth management businesses provide financial planning, investment management, banking and comprehensive advice to a wide range of clients, including ultra-high net worth and high net worth individuals, as well as family offices, foundations and endowments, and corporations and their employees. Our consumer business provides digital solutions for customers to better spend, borrow, invest, and save. Across Wealth Management, our growth is driven by a relentless focus on our people, our clients and customers, and leading-edge technology, data and design.

Your Impact
This person will be:
- responsible for expanding and optimizing our cloud based data pipeline architecture
- building robust data pipelines and reporting tools

Basic Qualifications
- 3+ years of experience in data processing & software engineering and can build high-quality, scalable data oriented products
- Experience on distributed data technologies (e.g. Hadoop, MapReduce, Spark, EMR, etc..) for building efficient, large-scale data pipelines
- Strong Software Engineering experience with in-depth understanding of Python, Scala, Java or equivalent
- Strong understanding of data architecture, modeling and infrastructure
- Experience with building workflows (ETL pipelines)
- Experience with SQL and optimizing queries
- Problem solver with attention to detail who can see complex problems in the data space through end to end
- Willingness to work in a fast paced environment
- MS/BS in Computer Science or relevant industry experience

Preferred Qualifications
- Experience building scalable applications on the Cloud (Amazon AWS, Google Cloud, etc..)
- Experience building stream-processing applications (Spark streaming, Apache-Flink, Kafka, etc..)
`;
